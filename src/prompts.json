[
  {
    "id": "50e8eee7-0bb7-4f2d-919c-ecdc2e170974",
    "prompt": "Test prompt",
    "timestamp": "2024-08-17T10:04:50.520535"
  },
  {
    "id": "d2af5f66-7e18-47f4-ac27-860458f3c780",
    "prompt": "Test prompt",
    "timestamp": "2024-08-17T10:04:17.350648"
  },
  {
    "id": "96d688ac-d43f-4526-b716-67510e12632e",
    "prompt": "Test prompt",
    "timestamp": "2024-08-17T09:34:19.731527"
  },
  {
    "id": "4b1bdd90-db11-48be-9d15-97b3379697d2",
    "prompt": "# Backend script configuration\nANACONDA_ENV_NAME=aider-ollama\nBackendScriptPath=<you path to repo>llama-cpp-chat\\src\\backend\\server.py\n\n# Frontend script configuration\nFrontendDirectoryPath=C:\\projects\\llama.cpp\\projects\\src\\llama-cpp-chat\n\n# Ollama script configuration\nANACONDA_ENV_NAME=aider-ollama\nOllamaPath=<you path to programms>\\Programs\\Ollama\\ollama.exe\nAPI_KEY_OPEN_AI=<apikey>\nPROJECT_MODEL_PATH=C:\\\\projects\\\\llama.cpp\\\\models\\\\custom\\\nSERVER_URL=http://localhost:5000\n",
    "timestamp": "2024-08-13T21:09:46.814921"
  }
]