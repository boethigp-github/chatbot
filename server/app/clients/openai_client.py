import json
from server.app.clients.base_client import BaseClient
from openai import OpenAI
from server.app.models.stream_response import StreamResponse
import time
from flask import current_app

class OpenAIClient(BaseClient):
    def __init__(self, api_key):
        if not api_key:
            raise ValueError("OpenAI API key is not set")
        self.client = OpenAI(api_key=api_key)

        self.cache = current_app.extensions['cache']



    def generate(self, prompt: str, **kwargs):
        """
        Generate a completion for a given prompt using the OpenAI API.

        Args:
            prompt (str): The prompt to send to the OpenAI model.
            **kwargs: Additional parameters like model, max_tokens, etc.

        Returns:
            str: The content generated by the model.
        """
        response = self.client.chat.completions.create(
            model=kwargs.get('model', 'gpt-3.5-turbo'),
            messages=[{"role": "user", "content": prompt}],
            max_tokens=kwargs.get('max_tokens', 1000),
            temperature=kwargs.get('temperature', 0.7),
        )
        return response.choices[0].message['content']




    def get_available_models(self):
        """
        Get a list of available models from OpenAI, with caching using cachetools.

        Returns:
            list: A list of model IDs available in the OpenAI account.
        """

        cached_models = self.cache.get('models')
        if cached_models:
            return self.cache.get('models')

        try:
            # List all models available in the account
            response = self.client.models.list()
            models = [model.id for model in response]
            # Cache the result
            self.cache.set('models', models, expire=3600)
            return models
        except Exception as e:

            raise ValueError(f"Error fetching models from OpenAI: {str(e)}")

    def stream_response(self, model, prompt, prompt_id):
        try:
            output = self.generate(
                prompt,
                model=model,
                max_tokens=1000,
                temperature=0.9
            )

            for token in output.split():

                stream_reponse = StreamResponse(status="data",
                                          token=token,
                                          message=output,
                                          model=model,
                                          prompt=prompt,
                                          prompt_id=prompt_id)



                yield stream_reponse.model_dump_json()
                time.sleep(0.1)

            yield StreamResponse(status="end").model_dump_json()
        except Exception as e:
            yield StreamResponse(status="error", message=f"OpenAI generation failed: {str(e)}").model_dump_json()